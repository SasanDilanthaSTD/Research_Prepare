{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f90545",
   "metadata": {},
   "source": [
    "#### load the nessary pacages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e5e032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For loading your trained ML model\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon, mapping\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx # For adding basemaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac7010",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a8b4e",
   "metadata": {},
   "source": [
    "<- define config file path\n",
    "\n",
    "```bash\n",
    "script_dir = Path(__file__).parent  # E:\\Research\\Research_Prepare\\src\\Sample\n",
    "script_dir  \n",
    "src_dir = script_dir.parent  # E:\\Research\\Research_Prepare\\src\n",
    "root_dir = src_dir.parent  # E:\\Research\\Research_Prepare\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b0a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"../../\")   # Adjust this path to your project root\n",
    "src_dir = root_dir / \"src\"  # Path to your source directory\n",
    "# Path to your multispectral UAV image\n",
    "UAV_IMAGE_PATH = root_dir / \"temp\" / \"odm_orthophoto\" / \"odm_orthophoto.tif\"\n",
    "if not UAV_IMAGE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"UAV image not found at {UAV_IMAGE_PATH}\")\n",
    "# Path to your trained ML model\n",
    "ML_MODEL_PATH = src_dir / \"model\" / \"xgb_model_v3.joblib\" \n",
    "if not ML_MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"ML model not found at {ML_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a011c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 64 # Pixels for square patches\n",
    "\n",
    "# Threshold for filtering \"informationless\" patches:\n",
    "# Sum of all pixel values across all bands in a patch.\n",
    "# Adjust this value based on your data. A very low sum indicates black/empty areas.\n",
    "MIN_PIXEL_SUM_THRESHOLD = 5000 # Example: Tune this value!\n",
    "\n",
    "# Define your growth stages in the order your model predicts them (0, 1, 2, ...)\n",
    "GROWTH_STAGES = [\"germination\", \"tillering\", \"grand_growth\", \"ripening\"]\n",
    "\n",
    "# Output GeoJSON file for map visualization\n",
    "OUTPUT_GEOJSON_PATH = \"sugarcane_growth_patches.geojson\"\n",
    "\n",
    "# Output image for the desktop map visualization\n",
    "OUTPUT_MAP_IMAGE_PATH = \"sugarcane_field_growth_map.png\"\n",
    "\n",
    "# Band mapping for your UAV image\n",
    "BAND_MAPPING = {\n",
    "    \"RED\": 0,\n",
    "    \"NIR\": 4,\n",
    "    \"SWIR\": 1\n",
    "}\n",
    "\n",
    "MODL_PATH = src_dir / \"model\" / \"xgb_model_v3.joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8e696",
   "metadata": {},
   "source": [
    "<<<--- testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ad19bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using band mapping: {'RED': 0, 'NIR': 4, 'SWIR': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using band mapping: {BAND_MAPPING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae11f9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (6, 1760, 1322)\n",
      "Image profile: {'driver': 'GTiff', 'dtype': 'uint16', 'nodata': None, 'width': 1322, 'height': 1760, 'count': 6, 'crs': CRS.from_wkt('PROJCS[\"WGS 84 / UTM zone 44N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",81],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32644\"]]'), 'transform': Affine(0.04998595505613276, 0.0, 575877.3194171236,\n",
      "       0.0, -0.04998374037942002, 804183.3428130195), 'blockxsize': 512, 'blockysize': 512, 'tiled': True, 'compress': 'deflate', 'interleave': 'pixel'}\n",
      "Band descriptions: ('Red', 'Green', 'Blue', 'NIR', 'RedEdge', None)\n"
     ]
    }
   ],
   "source": [
    "# Function to read the UAV image and extract bands\n",
    "with rasterio.open(UAV_IMAGE_PATH) as src:\n",
    "        image = src.read()  # Reads all bands into a NumPy array\n",
    "        profile = src.profile  # Save profile for GeoTIFF metadata\n",
    "        description = src.descriptions  # Band descriptions if available\n",
    "        \n",
    "print(f\"Image shape: {image.shape}\")  # Should be (bands, height, width)\n",
    "print(f\"Image profile: {profile}\")  # Check metadata for CRS, transform, etc.\n",
    "print(f\"Band descriptions: {description}\")  # Useful for understanding band order\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07de4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\research_env\\lib\\site-packages\\rasterio\\__init__.py:368: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<>>>>>>> Train patch band descriptions <<<<<<<<\n",
      "T1 description: RedEdge\n",
      "T2 description: Red\n",
      "T3 description: Green\n",
      ">>>>>>>> End of train patch band descriptions <<<<<<<<\n",
      "Train patch image shape: (5, 3887, 5277)\n",
      "Train patch profile: {'driver': 'GTiff', 'dtype': 'float32', 'nodata': None, 'width': 5277, 'height': 3887, 'count': 5, 'crs': None, 'transform': Affine(1.0, 0.0, 0.0,\n",
      "       0.0, 1.0, 0.0), 'blockxsize': 5277, 'blockysize': 1, 'tiled': False, 'interleave': 'pixel'}\n",
      "Train patch band descriptions: ('Blue', 'Green', 'Red', 'RedEdge', 'NIR')\n"
     ]
    }
   ],
   "source": [
    "train_on_of_img_dir = root_dir/\"data/MULTISPECTRAL/grand_growth/crop_main/grand_growth_main.tif\"\n",
    "\n",
    "if not train_on_of_img_dir.exists():\n",
    "    raise FileNotFoundError(f\"Patch image not found at {train_on_of_img_dir}\")\n",
    "\n",
    "with rasterio.open(train_on_of_img_dir) as src:\n",
    "    train_patch_img = src.read()  # Reads all bands into a NumPy array\n",
    "    train_patch_profile = src.profile  # Save profile for GeoTIFF metadata\n",
    "    train_patch_description = src.descriptions  # Band descriptions if available\n",
    "    \n",
    "    t1, t2, t3 = train_patch_img[3], train_patch_img[2], train_patch_img[1]  # Assuming NIR, RED, GREEN order\n",
    "    \n",
    "    print(\"<>>>>>>> Train patch band descriptions <<<<<<<<\")\n",
    "    print(f'T1 description: {train_patch_description[3]}')\n",
    "    print(f'T2 description: {train_patch_description[2]}')\n",
    "    print(f'T3 description: {train_patch_description[1]}')\n",
    "    print(\">>>>>>>> End of train patch band descriptions <<<<<<<<\")\n",
    "\n",
    "print(f\"Train patch image shape: {train_patch_img.shape}\")  # Should be (bands, height, width)\n",
    "print(f\"Train patch profile: {train_patch_profile}\")  # Check metadata for CRS, transform, etc.\n",
    "print(f\"Train patch band descriptions: {train_patch_description}\")  # Useful for understanding band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c16871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using corrected band mapping from Fix4D : {'BLUE': 0, 'GREEN': 1, 'RED': 2, 'RED_EDGE': 3, 'NIR': 4}\n",
      "Using corrected band mapping from ODM : {'RED': 0, 'GREEN': 1, 'BLUE': 2, 'NIR': 3, 'RED_EDGE': 4}\n"
     ]
    }
   ],
   "source": [
    "# Correct band mapping\n",
    "BAND_MAPPING_CORRECT_Fix4D = {\n",
    "    \"BLUE\": 0,\n",
    "    \"GREEN\": 1,\n",
    "    \"RED\": 2,\n",
    "    \"RED_EDGE\": 3,\n",
    "    \"NIR\": 4,\n",
    "}\n",
    "\n",
    "BAND_MAPPING_CORRECT_ODM = {\n",
    "    \"RED\": 0,\n",
    "    \"GREEN\": 1,\n",
    "    \"BLUE\": 2,\n",
    "    \"NIR\": 3,\n",
    "    \"RED_EDGE\": 4,\n",
    "}\n",
    "\n",
    "print(f\"Using corrected band mapping from Fix4D : {BAND_MAPPING_CORRECT_Fix4D}\")\n",
    "print(f\"Using corrected band mapping from ODM : {BAND_MAPPING_CORRECT_ODM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de00327",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "133e6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndvi(nir_band, red_band):\n",
    "    \"\"\"Calculates Normalized Difference Vegetation Index (NDVI).\"\"\"\n",
    "    return (nir_band - red_band) / (nir_band + red_band + 1e-10)\n",
    "\n",
    "def calculate_ndwi(nir_band, swir_band):\n",
    "    \"\"\"Calculates Normalized Difference Water Index (NDWI).\"\"\"\n",
    "    return (nir_band - swir_band) / (nir_band + swir_band + 1e-10)\n",
    "\n",
    "\n",
    "def extract_features_from_patch_array(patch_array):\n",
    "    \"\"\"\n",
    "    Extracts features from a single multispectral patch NumPy array.\n",
    "    Expects patch_array shape: (num_bands, height, width)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    # Safely get band data using BAND_MAPPING\n",
    "    try:\n",
    "        red_band = patch_array[BAND_MAPPING[\"RED\"]]\n",
    "        nir_band = patch_array[BAND_MAPPING[\"NIR\"]]\n",
    "        swir_band = patch_array[BAND_MAPPING[\"SWIR\"]]\n",
    "        \n",
    "        # Calculate NDVI and NDWI\n",
    "        ndvi = calculate_ndvi(nir_band, red_band)\n",
    "        ndwi = calculate_ndwi(nir_band, swir_band)\n",
    "        \n",
    "        features = [\n",
    "            np.mean(ndvi), np.std(ndvi),\n",
    "            np.mean(ndwi), np.std(ndwi),\n",
    "            np.percentile(nir_band, 75),\n",
    "            np.mean(swir_band > np.quantile(swir_band, 0.75))\n",
    "        ]\n",
    "        \n",
    "        return np.array(features)\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"Missing required band in patch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53213333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_field_for_mapping(image_path: Path, ml_model, growth_stages: list,\n",
    "                              patch_size: int = 64, min_pixel_sum_threshold: int = 1000) -> Path:\n",
    "    \"\"\"\n",
    "    Processes a multispectral GeoTIFF, extracts valid patches,\n",
    "    predicts growth stages in batch, and generates a GeoJSON file.\n",
    "\n",
    "    Args:\n",
    "        image_path (Path): Path to the input multispectral GeoTIFF.\n",
    "        ml_model: Loaded scikit-learn compatible ML model.\n",
    "        growth_stages (list): List of growth stage names corresponding to model's labels.\n",
    "        patch_size (int): Size of the square patches (e.g., 64).\n",
    "        min_pixel_sum_threshold (int): Patches with a total pixel sum below this\n",
    "                                       threshold will be considered \"informationless\" (black)\n",
    "                                       and skipped. Tune this value.\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the generated GeoJSON file.\n",
    "    \"\"\"\n",
    "    print(f\">>>>>>>>>>--------- Starting processing for: {image_path.name} ---------<<<<<<<<<<\", flush=True)\n",
    "    \n",
    "    patches_to_process = [] # Stores (patch_data, r_start, c_start)\n",
    "    \n",
    "    with rasterio.open(image_path) as src:\n",
    "        # Read all image data once for memory efficiency in patch extraction\n",
    "        # This assumes the image fits in memory. For extremely large images,\n",
    "        # you'd need to process by window.\n",
    "        image_data = src.read() \n",
    "        profile = src.profile\n",
    "        transform = src.transform\n",
    "        nodata_val = src.nodata\n",
    "        \n",
    "        bands, h, w = image_data.shape\n",
    "        print(f\"Image dimensions: {h}x{w} pixels, {bands} bands.\", flush=True)\n",
    "        \n",
    "        num_patches_skipped = 0\n",
    "        total_possible_patches = 0\n",
    "        i = 0  # Patch index for debugging\n",
    "\n",
    "        for r_start in range(0, h, patch_size):\n",
    "            for c_start in range(0, w, patch_size):\n",
    "                total_possible_patches += 1\n",
    "                r_end = r_start + patch_size\n",
    "                c_end = c_start + patch_size\n",
    "\n",
    "                # Ensure patch fits exactly within image bounds\n",
    "                # if r_end > h or c_end > w:\n",
    "                #     # Skip partial patches at the edges for simplicity.\n",
    "                #     # Alternatively, you could pad them to PATCH_SIZE if your model handles it.\n",
    "                #     num_patches_skipped += 1\n",
    "                #     continue\n",
    "\n",
    "                patch_data = image_data[:, r_start:r_end, c_start:c_end]\n",
    "                #print(patch_data.shape)\n",
    "                \n",
    "                # --- Filtering for \"informationless\" patches ---\n",
    "                # 1. Check for nodata values (if defined in GeoTIFF)\n",
    "                if nodata_val is not None and np.all(patch_data == nodata_val):\n",
    "                    #print(f\"----->{i} Skipping patch at {r_start},{c_start}: All pixels are nodata ({nodata_val}).\", flush=True)\n",
    "                    patches_to_process.append((patch_data, r_start, c_start, \"skip\"))\n",
    "                    num_patches_skipped += 1\n",
    "                    #print(f\"----->{i} num_patches_skipped: {num_patches_skipped} <-----\", flush=True)\n",
    "                    continue\n",
    "                \n",
    "                # # 2. Check if the patch is mostly black/very low intensity\n",
    "                if np.sum(patch_data) < min_pixel_sum_threshold:\n",
    "                    #print(f\"----->{i} Skipping patch at {r_start},{c_start}: Total pixel sum is too low ({np.sum(patch_data)}).\", flush=True)\n",
    "                    patches_to_process.append((patch_data, r_start, c_start, \"skip\"))\n",
    "                    num_patches_skipped += 1\n",
    "                    #print(f\"----->{i} num_patches_skipped: {num_patches_skipped} <-----\", flush=True)\n",
    "                    continue\n",
    "                \n",
    "                # # 3. Basic check for sufficient band data for feature extraction\n",
    "                if patch_data.shape[0] < max(BAND_MAPPING.values()) + 1:\n",
    "                    #print(f\"----->{i} Skipping patch at {r_start},{c_start}: Not enough bands ({patch_data.shape[0]}) for required indexing.\", flush=True)\n",
    "                    patches_to_process.append((patch_data, r_start, c_start, \"skip\"))\n",
    "                    num_patches_skipped += 1\n",
    "                    #print(f\"----->{i} num_patches_skipped: {num_patches_skipped} <-----\", flush=True)\n",
    "                    continue\n",
    "\n",
    "                patches_to_process.append((patch_data, r_start, c_start, \"valid\"))\n",
    "                i+= 1  # Increment patch index for debugging\n",
    "\n",
    "    valid_patches = [p for p in patches_to_process if p[3] == \"valid\"]           \n",
    "    #print(f\"valid patches to process: {len(valid_patches)} out of {total_possible_patches} possible patches.\", flush=True)\n",
    "                \n",
    "    print(f\"===> Found {len(valid_patches)} valid patches to process out of {total_possible_patches} possible patches.\", flush=True)\n",
    "    print(f\"===> Skipped {num_patches_skipped} informationless/partial patches.\", flush=True)\n",
    "    print(f\"===> Total patches to process: {len(patches_to_process)}\", flush=True)\n",
    "    \n",
    "    if not patches_to_process:\n",
    "        print(\"No valid patches found to process. Exiting.\", flush=True)\n",
    "        return None\n",
    "    \n",
    "    # --- Batch Feature Extraction ---\n",
    "    print(\"===> Extracting features in batch...\", flush=True)\n",
    "    # Extract features for all valid patches\n",
    "    # Use list comprehension for efficient feature extraction\n",
    "    all_features = []\n",
    "    for patch_data, r_start, c_start, status in patches_to_process:\n",
    "        if status == \"valid\":\n",
    "            try:\n",
    "                features = extract_features_from_patch_array(patch_data)\n",
    "                #print(f\"Extracted features for patch at {r_start},{c_start}: {features}\", flush=True)\n",
    "                # p[0] = features  # Replace patch data with extracted features\n",
    "                all_features.append(features)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error extracting features from patch at {p[1]},{p[2]}: {e}\", flush=True)\n",
    "    \n",
    "    #all_features = [extract_features_from_patch_array(p[0]) for p in patches_to_process if p[3] == \"valid\"]\n",
    "    \n",
    "    # Filter out patches where feature extraction failed (returned NaNs)\n",
    "    valid_features_and_indices = []\n",
    "    for i, features in enumerate(all_features):\n",
    "        if not any(np.isnan(f) for f in features): # Check if any feature is NaN\n",
    "            valid_features_and_indices.append((features, i))\n",
    "        else:\n",
    "            print(f\"Skipping patch {i} due to invalid features (NaN/Inf).\", flush=True)\n",
    "    #valid_features_and_indices.append((features, i))\n",
    "    # print(f\"Extracted features for {len(valid_features_and_indices)} valid patches.\", flush=True)\n",
    "    # print(valid_features_and_indices)\n",
    "\n",
    "    if not valid_features_and_indices:\n",
    "        print(\"No valid features extracted after filtering. Exiting.\", flush=True)\n",
    "        return None\n",
    "\n",
    "    # Separate features and original indices\n",
    "    features_for_prediction = np.array([item[0] for item in valid_features_and_indices])\n",
    "    original_patch_indices = [item[1] for item in valid_features_and_indices]\n",
    "    \n",
    "    print(f\"===> Successfully extracted features for {len(features_for_prediction)} patches.\", flush=True)\n",
    "    \n",
    "    # --- Batch Prediction ---\n",
    "    print(\"===> Performing batch prediction...\", flush=True)\n",
    "    # print(f\"Features shape: {features_for_prediction.shape}\", flush=True)\n",
    "    # print(features_for_prediction)\n",
    "    predictions_labels = []\n",
    "    for i, features in enumerate(features_for_prediction):\n",
    "        # print(f\"Predicting for patch {i} with features: {features}\", flush=True)\n",
    "        try:\n",
    "            label = ml_model.predict([features])[0]  # Predict single patch\n",
    "            predictions_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting for patch {i}: {e}\", flush=True)\n",
    "            predictions_labels.append(-1)\n",
    "    # predictions_labels = ml_model.predict(features_for_prediction)\n",
    "    predictions_stages = [growth_stages[label] for label in predictions_labels]\n",
    "    \n",
    "    #print(predictions_stages)\n",
    "    \n",
    "    print(\"===> Prediction complete. Next generating GeoJSON.\", flush=True)\n",
    "    \n",
    "    # --- GeoJSON Generation ---\n",
    "    geojson_features = []\n",
    "    with rasterio.open(image_path) as src:\n",
    "        transform = src.transform\n",
    "\n",
    "        prediction_index_map = dict(zip(original_patch_indices, predictions_stages))  # Map index → predicted stage\n",
    "        # print(f\"----> Prediction index map: {prediction_index_map}\", flush=True)\n",
    "        # print(f\"----> Generating GeoJSON features for {len(patches_to_process)} patches.\", flush=True)\n",
    "\n",
    "        for i, patch in enumerate(patches_to_process):\n",
    "            _patch_data, r_start, c_start, status = patch\n",
    "\n",
    "            # print(f\"{i} -- Patch data shape: {_patch_data.shape}\", flush=True)\n",
    "            # print(f\"{i} -- Processing patch at {r_start},{c_start} with status: {status}\", flush=True)\n",
    "\n",
    "            # Calculate geo-coordinates of patch corners\n",
    "            ul_lon, ul_lat = transform * (c_start, r_start)\n",
    "            ur_lon, ur_lat = transform * (c_start + patch_size, r_start)\n",
    "            lr_lon, lr_lat = transform * (c_start + patch_size, r_start + patch_size)\n",
    "            ll_lon, ll_lat = transform * (c_start, r_start + patch_size)\n",
    "\n",
    "            patch_polygon = Polygon([\n",
    "                (ul_lon, ul_lat),\n",
    "                (ur_lon, ur_lat),\n",
    "                (lr_lon, lr_lat),\n",
    "                (ll_lon, ll_lat),\n",
    "                (ul_lon, ul_lat)  # Close the polygon\n",
    "            ])\n",
    "\n",
    "            # Set growth stage from prediction or None if skipped\n",
    "            growth_stage = prediction_index_map.get(i, None)\n",
    "            # print(f\"Patch {i} at {r_start},{c_start} has growth stage: {growth_stage}\", flush=True)\n",
    "            # print(f\"Patch Status: {status}\", flush=True)\n",
    "\n",
    "            geojson_features.append({\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": mapping(patch_polygon),\n",
    "                \"properties\": {\n",
    "                    \"growth_stage\": growth_stage,\n",
    "                    \"row_start\": r_start,\n",
    "                    \"col_start\": c_start,\n",
    "                }\n",
    "            })\n",
    "\n",
    "            \n",
    "    # print(f\"===> Generated {len(geojson_features)} GeoJSON features.\", flush=True)\n",
    "    # print(geojson_features)\n",
    "    \n",
    "    \n",
    "    output_geojson_data = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": geojson_features\n",
    "    }\n",
    "\n",
    "    with open(OUTPUT_GEOJSON_PATH, \"w\") as f:\n",
    "        json.dump(output_geojson_data, f, indent=2)\n",
    "\n",
    "    print(f\"----> GeoJSON data saved to {OUTPUT_GEOJSON_PATH}\", flush=True)\n",
    "    print(\"===> Processing complete. GeoJSON file generated.\", flush=True)\n",
    "    return Path(OUTPUT_GEOJSON_PATH)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c293b47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      ">>>>>>>>>>--------- Starting processing for: odm_orthophoto.tif ---------<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimensions: 1760x1322 pixels, 6 bands.\n",
      "===> Found 253 valid patches to process out of 588 possible patches.\n",
      "===> Skipped 335 informationless/partial patches.\n",
      "===> Total patches to process: 588\n",
      "===> Extracting features in batch...\n",
      "===> Successfully extracted features for 253 patches.\n",
      "===> Performing batch prediction...\n",
      "===> Prediction complete. Next generating GeoJSON.\n",
      "----> GeoJSON data saved to sugarcane_growth_patches.geojson\n",
      "===> Processing complete. GeoJSON file generated.\n",
      "sugarcane_growth_patches.geojson\n"
     ]
    }
   ],
   "source": [
    "#load the ML model\n",
    "if not MODL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Model not found at {MODL_PATH}\")\n",
    "ml_model = joblib.load(MODL_PATH)\n",
    "print(\"====================================================\")\n",
    "print(process_field_for_mapping(UAV_IMAGE_PATH, ml_model, GROWTH_STAGES, PATCH_SIZE, MIN_PIXEL_SUM_THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f587aaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
