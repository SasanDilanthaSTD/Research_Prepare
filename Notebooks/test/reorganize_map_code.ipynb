{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25856652",
   "metadata": {},
   "source": [
    ">- ### Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8fbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from matplotlib.colors import ListedColormap # for define a categorical colormap\n",
    "\n",
    "import contextily as cx\n",
    "\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7506d",
   "metadata": {},
   "source": [
    ">- define path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c008da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"../../\")   # Adjust this path to your project root\n",
    "src_dir = root_dir / \"src\"  # Path to your source directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc5838",
   "metadata": {},
   "source": [
    ">- filter nessary code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2aec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL_CRS = \"\"\n",
    "\n",
    "# geo_tiff_path = root_dir / \"temp\" / \"odm_orthophoto\" / \"odm_orthophoto.tif\"\n",
    "# # if the file exists,\n",
    "# if geo_tiff_path.exists():\n",
    "#     print(f\"GeoTIFF file found: {geo_tiff_path}\")\n",
    "# else:\n",
    "#     print(f\"GeoTIFF file not found: {geo_tiff_path}\")\n",
    "    \n",
    "# with rasterio.open(geo_tiff_path) as src:\n",
    "#     print(\"GeoTIFF CRS:\", src.crs)\n",
    "#     ORIGINAL_CRS = src.crs.to_string()\n",
    "    \n",
    "# print(\"Original CRS:\", ORIGINAL_CRS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a00e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRS: EPSG:32644\n",
      "Total Bounds: [575877.31941712 804093.77195026 575944.50054072 804183.34281302]\n",
      "Growth Stage Values: ['grand_growth' None]\n",
      "Number of Features: 588\n"
     ]
    }
   ],
   "source": [
    "# Load the GeoJSON file\n",
    "geo_json_obj = gpd.read_file(\"sugarcane_growth_patches.geojson\")\n",
    "\n",
    "# Assign the CRS (replace with the correct EPSG code from your GeoTIFF)\n",
    "geo_json_obj = geo_json_obj.set_crs(ORIGINAL_CRS, allow_override=True)\n",
    "\n",
    "# Verify CRS and bounds\n",
    "print(\"CRS:\", geo_json_obj.crs)\n",
    "print(\"Total Bounds:\", geo_json_obj.total_bounds)\n",
    "print(\"Growth Stage Values:\", geo_json_obj['growth_stage'].unique())\n",
    "print(\"Number of Features:\", len(geo_json_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e709f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive map saved as {save_path}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12140\\4070514451.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = geo_json_obj.to_crs(\"EPSG:4326\").geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "# Create a Folium map centered on the centroid of the patches\n",
    "centroid = geo_json_obj.to_crs(\"EPSG:4326\").geometry.centroid\n",
    "m = folium.Map(location=[centroid.y.mean(), centroid.x.mean()], zoom_start=16, tiles=\"OpenStreetMap\")\n",
    "\n",
    "# Add GeoJSON to the map with style based on growth_stage\n",
    "folium.GeoJson(\n",
    "    geo_json_obj.to_crs(\"EPSG:4326\"),\n",
    "    style_function=lambda x: {\n",
    "        'fillColor': '#FF0000' if x['properties']['growth_stage'] == 'grand_growth' else '#808080',\n",
    "        'color': 'black',\n",
    "        'weight': 0.2,\n",
    "        'fillOpacity': 0.7\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(fields=['growth_stage', 'row_start', 'col_start'])\n",
    ").add_to(m)\n",
    "\n",
    "# define save file path\n",
    "save_dir = root_dir / \"temp_map\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
    "save_path = save_dir / \"sugarcane_growth_map.html\"\n",
    "# Save and display\n",
    "m.save(save_path)\n",
    "print(\"Interactive map saved as {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010934a8",
   "metadata": {},
   "source": [
    "### GEOJSON create code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_field_for_mapping(image_path: Path, ml_model, growth_stages: list,\n",
    "                              patch_size: int = 64, min_pixel_sum_threshold: int = 1000) -> Path:\n",
    "    \"\"\"\n",
    "    Processes a multispectral GeoTIFF, extracts valid patches,\n",
    "    predicts growth stages in batch, and generates a GeoJSON file.\n",
    "\n",
    "    Args:\n",
    "        image_path (Path): Path to the input multispectral GeoTIFF.\n",
    "        ml_model: Loaded scikit-learn compatible ML model.\n",
    "        growth_stages (list): List of growth stage names corresponding to model's labels.\n",
    "        patch_size (int): Size of the square patches (e.g., 64).\n",
    "        min_pixel_sum_threshold (int): Patches with a total pixel sum below this\n",
    "                                       threshold will be considered \"informationless\" (black)\n",
    "                                       and skipped. Tune this value.\n",
    "\n",
    "    Returns:\n",
    "        Path: Path to the generated GeoJSON file.\n",
    "    \"\"\"\n",
    "    print(f\">>>>>>>>>>--------- Starting processing for: {image_path.name} ---------<<<<<<<<<<\", flush=True)\n",
    "    \n",
    "    patches_to_process = [] # Stores (patch_data, r_start, c_start)\n",
    "    \n",
    "    with rasterio.open(image_path) as src:\n",
    "        # Read all image data once for memory efficiency in patch extraction\n",
    "        # This assumes the image fits in memory. For extremely large images,\n",
    "        # you'd need to process by window.\n",
    "        image_data = src.read() \n",
    "        profile = src.profile\n",
    "        transform = src.transform\n",
    "        nodata_val = src.nodata\n",
    "        \n",
    "        bands, h, w = image_data.shape\n",
    "        print(f\"Image dimensions: {h}x{w} pixels, {bands} bands.\", flush=True)\n",
    "        \n",
    "        num_patches_skipped = 0\n",
    "        total_possible_patches = 0\n",
    "        i = 0  # Patch index for debugging\n",
    "\n",
    "        for r_start in range(0, h, patch_size):\n",
    "            for c_start in range(0, w, patch_size):\n",
    "                total_possible_patches += 1\n",
    "                r_end = r_start + patch_size\n",
    "                c_end = c_start + patch_size\n",
    "\n",
    "                # Ensure patch fits exactly within image bounds\n",
    "                # if r_end > h or c_end > w:\n",
    "                #     # Skip partial patches at the edges for simplicity.\n",
    "                #     # Alternatively, you could pad them to PATCH_SIZE if your model handles it.\n",
    "                #     num_patches_skipped += 1\n",
    "                #     continue\n",
    "\n",
    "                patch_data = image_data[:, r_start:r_end, c_start:c_end]\n",
    "                #print(patch_data.shape)\n",
    "                \n",
    "                # --- Filtering for \"informationless\" patches ---\n",
    "                # 1. Check for nodata values (if defined in GeoTIFF)\n",
    "                if nodata_val is not None and np.all(patch_data == nodata_val):\n",
    "                    #print(f\"----->{i} Skipping patch at {r_start},{c_start}: All pixels are nodata ({nodata_val}).\", flush=True)\n",
    "                    patches_to_process.append((patch_data, r_start, c_start, \"skip\"))\n",
    "                    num_patches_skipped += 1\n",
    "                    #print(f\"----->{i} num_patches_skipped: {num_patches_skipped} <-----\", flush=True)\n",
    "                    continue\n",
    "                \n",
    "                # # 2. Check if the patch is mostly black/very low intensity\n",
    "                if np.sum(patch_data) < min_pixel_sum_threshold:\n",
    "                    #print(f\"----->{i} Skipping patch at {r_start},{c_start}: Total pixel sum is too low ({np.sum(patch_data)}).\", flush=True)\n",
    "                    patches_to_process.append((patch_data, r_start, c_start, \"skip\"))\n",
    "                    num_patches_skipped += 1\n",
    "                    #print(f\"----->{i} num_patches_skipped: {num_patches_skipped} <-----\", flush=True)\n",
    "                    continue\n",
    "                \n",
    "                # # 3. Basic check for sufficient band data for feature extraction\n",
    "                if patch_data.shape[0] < max(BAND_MAPPING.values()) + 1:\n",
    "                    #print(f\"----->{i} Skipping patch at {r_start},{c_start}: Not enough bands ({patch_data.shape[0]}) for required indexing.\", flush=True)\n",
    "                    patches_to_process.append((patch_data, r_start, c_start, \"skip\"))\n",
    "                    num_patches_skipped += 1\n",
    "                    #print(f\"----->{i} num_patches_skipped: {num_patches_skipped} <-----\", flush=True)\n",
    "                    continue\n",
    "\n",
    "                patches_to_process.append((patch_data, r_start, c_start, \"valid\"))\n",
    "                i+= 1  # Increment patch index for debugging\n",
    "\n",
    "    valid_patches = [p for p in patches_to_process if p[3] == \"valid\"]           \n",
    "    #print(f\"valid patches to process: {len(valid_patches)} out of {total_possible_patches} possible patches.\", flush=True)\n",
    "                \n",
    "    print(f\"===> Found {len(valid_patches)} valid patches to process out of {total_possible_patches} possible patches.\", flush=True)\n",
    "    print(f\"===> Skipped {num_patches_skipped} informationless/partial patches.\", flush=True)\n",
    "    print(f\"===> Total patches to process: {len(patches_to_process)}\", flush=True)\n",
    "    \n",
    "    if not patches_to_process:\n",
    "        print(\"No valid patches found to process. Exiting.\", flush=True)\n",
    "        return None\n",
    "    \n",
    "    # --- Batch Feature Extraction ---\n",
    "    print(\"===> Extracting features in batch...\", flush=True)\n",
    "    # Extract features for all valid patches\n",
    "    # Use list comprehension for efficient feature extraction\n",
    "    all_features = []\n",
    "    for patch_data, r_start, c_start, status in patches_to_process:\n",
    "        if status == \"valid\":\n",
    "            try:\n",
    "                features = extract_features_from_patch_array(patch_data)\n",
    "                #print(f\"Extracted features for patch at {r_start},{c_start}: {features}\", flush=True)\n",
    "                # p[0] = features  # Replace patch data with extracted features\n",
    "                all_features.append(features)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error extracting features from patch at {p[1]},{p[2]}: {e}\", flush=True)\n",
    "    \n",
    "    #all_features = [extract_features_from_patch_array(p[0]) for p in patches_to_process if p[3] == \"valid\"]\n",
    "    \n",
    "    # Filter out patches where feature extraction failed (returned NaNs)\n",
    "    valid_features_and_indices = []\n",
    "    for i, features in enumerate(all_features):\n",
    "        if not any(np.isnan(f) for f in features): # Check if any feature is NaN\n",
    "            valid_features_and_indices.append((features, i))\n",
    "        else:\n",
    "            print(f\"Skipping patch {i} due to invalid features (NaN/Inf).\", flush=True)\n",
    "    #valid_features_and_indices.append((features, i))\n",
    "    # print(f\"Extracted features for {len(valid_features_and_indices)} valid patches.\", flush=True)\n",
    "    # print(valid_features_and_indices)\n",
    "\n",
    "    if not valid_features_and_indices:\n",
    "        print(\"No valid features extracted after filtering. Exiting.\", flush=True)\n",
    "        return None\n",
    "\n",
    "    # Separate features and original indices\n",
    "    features_for_prediction = np.array([item[0] for item in valid_features_and_indices])\n",
    "    original_patch_indices = [item[1] for item in valid_features_and_indices]\n",
    "    \n",
    "    print(f\"===> Successfully extracted features for {len(features_for_prediction)} patches.\", flush=True)\n",
    "    \n",
    "    # --- Batch Prediction ---\n",
    "    print(\"===> Performing batch prediction...\", flush=True)\n",
    "    # print(f\"Features shape: {features_for_prediction.shape}\", flush=True)\n",
    "    # print(features_for_prediction)\n",
    "    predictions_labels = []\n",
    "    for i, features in enumerate(features_for_prediction):\n",
    "        # print(f\"Predicting for patch {i} with features: {features}\", flush=True)\n",
    "        try:\n",
    "            label = ml_model.predict([features])[0]  # Predict single patch\n",
    "            predictions_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting for patch {i}: {e}\", flush=True)\n",
    "            predictions_labels.append(-1)\n",
    "    # predictions_labels = ml_model.predict(features_for_prediction)\n",
    "    predictions_stages = [growth_stages[label] for label in predictions_labels]\n",
    "    \n",
    "    #print(predictions_stages)\n",
    "    \n",
    "    print(\"===> Prediction complete. Next generating GeoJSON.\", flush=True)\n",
    "    \n",
    "    # --- GeoJSON Generation ---\n",
    "    geojson_features = []\n",
    "    with rasterio.open(image_path) as src:\n",
    "        transform = src.transform\n",
    "\n",
    "        prediction_index_map = dict(zip(original_patch_indices, predictions_stages))  # Map index → predicted stage\n",
    "        # print(f\"----> Prediction index map: {prediction_index_map}\", flush=True)\n",
    "        # print(f\"----> Generating GeoJSON features for {len(patches_to_process)} patches.\", flush=True)\n",
    "\n",
    "        for i, patch in enumerate(patches_to_process):\n",
    "            _patch_data, r_start, c_start, status = patch\n",
    "\n",
    "            # print(f\"{i} -- Patch data shape: {_patch_data.shape}\", flush=True)\n",
    "            # print(f\"{i} -- Processing patch at {r_start},{c_start} with status: {status}\", flush=True)\n",
    "\n",
    "            # Calculate geo-coordinates of patch corners\n",
    "            ul_lon, ul_lat = transform * (c_start, r_start)\n",
    "            ur_lon, ur_lat = transform * (c_start + patch_size, r_start)\n",
    "            lr_lon, lr_lat = transform * (c_start + patch_size, r_start + patch_size)\n",
    "            ll_lon, ll_lat = transform * (c_start, r_start + patch_size)\n",
    "\n",
    "            patch_polygon = Polygon([\n",
    "                (ul_lon, ul_lat),\n",
    "                (ur_lon, ur_lat),\n",
    "                (lr_lon, lr_lat),\n",
    "                (ll_lon, ll_lat),\n",
    "                (ul_lon, ul_lat)  # Close the polygon\n",
    "            ])\n",
    "\n",
    "            # Set growth stage from prediction or None if skipped\n",
    "            growth_stage = prediction_index_map.get(i, None)\n",
    "            # print(f\"Patch {i} at {r_start},{c_start} has growth stage: {growth_stage}\", flush=True)\n",
    "            # print(f\"Patch Status: {status}\", flush=True)\n",
    "\n",
    "            geojson_features.append({\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": mapping(patch_polygon),\n",
    "                \"properties\": {\n",
    "                    \"growth_stage\": growth_stage,\n",
    "                    \"row_start\": r_start,\n",
    "                    \"col_start\": c_start,\n",
    "                }\n",
    "            })\n",
    "\n",
    "            \n",
    "    # print(f\"===> Generated {len(geojson_features)} GeoJSON features.\", flush=True)\n",
    "    # print(geojson_features)\n",
    "    \n",
    "    \n",
    "    output_geojson_data = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"crs\": {\n",
    "            \"type\": \"name\",\n",
    "            \"properties\": {\"name\": f\"EPSG:{src.crs.to_epsg()}\"}\n",
    "        },\n",
    "        \"features\": geojson_features\n",
    "    }\n",
    "\n",
    "    with open(OUTPUT_GEOJSON_PATH, \"w\") as f:\n",
    "        json.dump(output_geojson_data, f, indent=2)\n",
    "\n",
    "    print(f\"----> GeoJSON data saved to {OUTPUT_GEOJSON_PATH}\", flush=True)\n",
    "    print(\"===> Processing complete. GeoJSON file generated.\", flush=True)\n",
    "    return Path(OUTPUT_GEOJSON_PATH)                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
